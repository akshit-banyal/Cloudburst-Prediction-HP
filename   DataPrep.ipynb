{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d02c2fc7-45c1-4945-9c2d-15a6279ab281",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'rasterio'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrasterio\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mrasterio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmerge\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m merge\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mrasterio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mplot\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m show\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'rasterio'"
     ]
    }
   ],
   "source": [
    "import rasterio\n",
    "from rasterio.merge import merge\n",
    "from rasterio.plot import show\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# The * is a wildcard that matches any characters\n",
    "dem_files = glob.glob(\"P5_PAN_CD*.tif\")\n",
    "\n",
    "# Create a list of rasterio dataset objects to merge\n",
    "src_files_to_mosaic = []\n",
    "for fp in dem_files:\n",
    "    src = rasterio.open(fp)\n",
    "    src_files_to_mosaic.append(src)\n",
    "\n",
    "# The merge function combines the rasters\n",
    "mosaic, out_trans = merge(src_files_to_mosaic)\n",
    "\n",
    "# Define the metadata for the output file\n",
    "out_meta = src.meta.copy()\n",
    "out_meta.update({\"driver\": \"GTiff\",\n",
    "                 \"height\": mosaic.shape[1],\n",
    "                 \"width\": mosaic.shape[2],\n",
    "                 \"transform\": out_trans,\n",
    "                 })\n",
    "\n",
    "# Write the merged raster to a new file\n",
    "output_filename = \"Kullu_Mandi_DEM_merged.tif\"\n",
    "with rasterio.open(output_filename, \"w\", **out_meta) as dest:\n",
    "    dest.write(mosaic)\n",
    "\n",
    "print(f\"Successfully merged {len(dem_files)} files into {output_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61fdecbf-a057-46b1-a420-7f866598fbd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rasterio\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Open the merged DEM file\n",
    "with rasterio.open('Kullu_Mandi_DEM_merged.tif') as dem:\n",
    "    # Read the elevation data as a numpy array\n",
    "    # We specify float32 to allow for NaN values\n",
    "    elevation_data = dem.read(1, masked=False).astype('float32')\n",
    "    \n",
    "    # Get the no-data value from the file's metadata\n",
    "    nodata_value = dem.nodata\n",
    "    \n",
    "    # Replace the no-data values with np.nan\n",
    "    if nodata_value is not None:\n",
    "        elevation_data[elevation_data == nodata_value] = np.nan\n",
    "        \n",
    "    # Get the geographic extent for plotting\n",
    "    extent = rasterio.plot.plotting_extent(dem)\n",
    "\n",
    "# Now plot the cleaned data using matplotlib's imshow\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 10))\n",
    "\n",
    "# Use imshow to display the data. Matplotlib automatically ignores NaN values.\n",
    "image = ax.imshow(elevation_data, cmap='terrain', extent=extent)\n",
    "\n",
    "# Add a colorbar to show the elevation scale\n",
    "cbar = fig.colorbar(image, ax=ax, shrink=0.7)\n",
    "cbar.set_label('Elevation (meters)')\n",
    "\n",
    "# Add titles and labels\n",
    "ax.set_title(\"Elevation Map of Kullu & Mandi Region\", fontsize=16)\n",
    "ax.set_xlabel(\"Longitude\")\n",
    "ax.set_ylabel(\"Latitude\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918c83f5-02fe-4d3a-8521-28027c79afa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rasterio\n",
    "import pandas as pd\n",
    "\n",
    "# Part 1: Extract Elevation from Your DEM File \n",
    "\n",
    "# Using your successful output as a hardcoded value\n",
    "# You don't need to run the rasterio part again if you have the value\n",
    "elevation_value = 1419.93 \n",
    "print(f\"Using pre-calculated elevation: {elevation_value:.2f} meters.\")\n",
    "\n",
    "print(\"\\n--------------------------------------\\n\")\n",
    "\n",
    "\n",
    "# Part 2: Load and Prepare Your DataFrames \n",
    "\n",
    "print(\"Loading your weather and events data...\")\n",
    "# Using your specific weather data filename\n",
    "weather_df = pd.read_csv(\"HP_Cloudburst_Data_2010-2024.csv\")\n",
    "\n",
    "# Using your specific historical events filename\n",
    "# FIX 1: Added 'on_bad_lines='skip' to handle the ParserError\n",
    "events_df = pd.read_csv(\"himachal_cloudburst_past_data.csv\", on_bad_lines='skip')\n",
    "\n",
    "# Create a standard 'date' column in both dataframes\n",
    "# FIX 2: Changed to use 'DOY' (Day of Year) instead of 'MO' and 'DY'\n",
    "weather_df['date'] = pd.to_datetime(weather_df['YEAR'].astype(str) + '-' + weather_df['DOY'].astype(str), format='%Y-%j')\n",
    "events_df['date'] = pd.to_datetime(events_df['Date'], errors='coerce') # 'errors=coerce' will handle any bad dates in this file too\n",
    "\n",
    "\n",
    "# Part 3: Combine and Label \n",
    "\n",
    "# 1. Add the elevation as a new column (feature)\n",
    "weather_df['elevation'] = elevation_value\n",
    "\n",
    "# 2. Create the target column, starting with all zeros\n",
    "weather_df['is_cloudburst'] = 0\n",
    "\n",
    "# 3. Use the events list to \"label\" the correct days with a '1'\n",
    "# Drop any rows from events_df where the date couldn't be read\n",
    "events_df = events_df.dropna(subset=['date']) \n",
    "event_dates = events_df['date'].tolist()\n",
    "\n",
    "weather_df.loc[weather_df['date'].isin(event_dates), 'is_cloudburst'] = 1\n",
    "\n",
    "\n",
    "# Final Step: Save and Verify \n",
    "\n",
    "output_filename = \"cloud_burst_dataset.csv\"\n",
    "weather_df.to_csv(output_filename, index=False)\n",
    "\n",
    "print(f\"\\nSUCCESS! Your final dataset is saved as: {output_filename}\")\n",
    "\n",
    "print(\"\\nVerification:\")\n",
    "print(\"This shows how many normal days (0) and cloudburst days (1) are in your final dataset.\")\n",
    "# This will show you the count of 0s (normal days) and 1s (cloudburst days)\n",
    "print(weather_df['is_cloudburst'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895770e8-916d-4bb6-92e0-d85f155e526d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "try:\n",
    "    # Load the weather data\n",
    "    weather_df = pd.read_csv(\"HP_Cloudburst_Data_2010-2024.csv\")\n",
    "    \n",
    "    # Load the events data, skipping the bad lines\n",
    "    events_df = pd.read_csv(\"himachal_cloudburst_past_data.csv\", on_bad_lines='skip')\n",
    "\n",
    "    # --- Convert Dates (as before) ---\n",
    "    weather_df['date'] = pd.to_datetime(weather_df['YEAR'].astype(str) + '-' + weather_df['DOY'].astype(str), format='%Y-%j')\n",
    "    events_df['date'] = pd.to_datetime(events_df['Date'], errors='coerce')\n",
    "\n",
    "    # --- Diagnostic Step: Check Date Ranges ---\n",
    "    print(\"--- Weather Data Date Range ---\")\n",
    "    print(f\"Min date: {weather_df['date'].min()}\")\n",
    "    print(f\"Max date: {weather_df['date'].max()}\")\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    # --- Filter for the \"Broad Definition\" events *before* checking dates ---\n",
    "    broad_definition_list = [\n",
    "        'Floods and Heavy Rain', 'Flash floods', 'Cloudburst', \n",
    "        'Cloudburst and flash flood', 'Flash flood and landslide', \n",
    "        'Flash floods after lake burst', 'Flood', 'Flash flood after cloudburst', \n",
    "        'Floods', 'Flash floods due to cloudbursts', 'Flash flood due to lake breach'\n",
    "    ]\n",
    "    \n",
    "    # Drop rows where date couldn't be read\n",
    "    events_df = events_df.dropna(subset=['date']) \n",
    "    # Filter for our events\n",
    "    cloudburst_events_df = events_df[events_df['Associated_Event'].isin(broad_definition_list)]\n",
    "\n",
    "    print(\"--- Event Data (Broad Definition) Date Range ---\")\n",
    "    if not cloudburst_events_df.empty:\n",
    "        print(f\"Min date: {cloudburst_events_df['date'].min()}\")\n",
    "        print(f\"Max date: {cloudburst_events_df['date'].max()}\")\n",
    "        print(f\"\\nFound {len(cloudburst_events_df)} relevant events to check.\")\n",
    "    else:\n",
    "        print(\"No events found matching the broad definition.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246551d8-9b1b-43d4-8405-dcf838c0a902",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# This is the single, corrected script.\n",
    "# It uses the \"Broad Definition\" AND the date normalization fix.\n",
    "\n",
    "print(\"--- Step 1: Loading Raw Data ---\")\n",
    "\n",
    "try:\n",
    "    # Load the weather data\n",
    "    weather_df = pd.read_csv(\"HP_Cloudburst_Data_2010-2024.csv\")\n",
    "    print(\"Loaded 'HP_Cloudburst_Data_2010-2024.csv'\")\n",
    "    \n",
    "    # Load the events data, skipping the bad lines\n",
    "    events_df = pd.read_csv(\"himachal_cloudburst_past_data.csv\", on_bad_lines='skip')\n",
    "    print(\"Loaded 'himachal_cloudburst_past_data.csv' (skipped bad lines)\")\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error: Could not find a required file: {e}\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during loading: {e}\")\n",
    "    exit()\n",
    "\n",
    "print(\"\\n--- Step 2: Data Preparation & Labeling (Broad Definition) ---\")\n",
    "\n",
    "# --- Date Conversion ---\n",
    "try:\n",
    "    weather_df['date'] = pd.to_datetime(weather_df['YEAR'].astype(str) + '-' + weather_df['DOY'].astype(str), format='%Y-%j')\n",
    "    events_df['date'] = pd.to_datetime(events_df['Date'], errors='coerce')\n",
    "    print(\"Converted date columns.\")\n",
    "except KeyError:\n",
    "    print(\"Error: Date columns not found.\")\n",
    "    exit()\n",
    "\n",
    "# --- *** THE CRITICAL FIX *** ---\n",
    "# Normalize both date columns to remove timestamps and ensure they match.\n",
    "weather_df['date'] = weather_df['date'].dt.normalize()\n",
    "events_df['date'] = events_df['date'].dt.normalize()\n",
    "print(\"Normalized dates (removed timestamps) for accurate matching.\")\n",
    "# --- *** END OF FIX *** ---\n",
    "\n",
    "# --- Add Topography Data ---\n",
    "elevation_value = 1419.93 \n",
    "weather_df['elevation'] = elevation_value\n",
    "\n",
    "# --- Labeling the Target Variable ---\n",
    "weather_df['is_cloudburst'] = 0\n",
    "\n",
    "# Define the \"Broad Definition\" list\n",
    "broad_definition_list = [\n",
    "    'Floods and Heavy Rain', 'Flash floods', 'Cloudburst', \n",
    "    'Cloudburst and flash flood', 'Flash flood and landslide', \n",
    "    'Flash floods after lake burst', 'Flood', 'Flash flood after cloudburst', \n",
    "    'Floods', 'Flash floods due to cloudbursts', 'Flash flood due to lake breach'\n",
    "]\n",
    "\n",
    "# Get a clean list of cloudburst event dates\n",
    "events_df = events_df.dropna(subset=['date']) \n",
    "cloudburst_events_df = events_df[events_df['Associated_Event'].isin(broad_definition_list)]\n",
    "# Get a list of *unique* dates\n",
    "event_dates = cloudburst_events_df['date'].unique().tolist()\n",
    "\n",
    "# Use the events list to \"label\" the correct days with a '1'\n",
    "weather_df.loc[weather_df['date'].isin(event_dates), 'is_cloudburst'] = 1\n",
    "\n",
    "print(\"\\n*** Verification of Labeled Data ***\")\n",
    "print(\"This output MUST show a count for '1's:\")\n",
    "print(weather_df['is_cloudburst'].value_counts())\n",
    "\n",
    "# Check how many events we found\n",
    "num_events = (weather_df['is_cloudburst'] == 1).sum()\n",
    "if num_events == 0:\n",
    "    print(\"\\nFATAL ERROR: Still no events found. Stopping.\")\n",
    "    exit()\n",
    "else:\n",
    "    print(f\"\\nSuccessfully labeled {num_events} cloudburst/flash flood event days.\")\n",
    "\n",
    "\n",
    "print(\"\\n--- Step 3: Feature Engineering (Lags & Rolling Averages) ---\")\n",
    "\n",
    "# Set 'date' as the index for time-series operations\n",
    "df = weather_df.set_index('date')\n",
    "df = df.sort_index()\n",
    "print(f\"Data sorted by date. Original shape: {df.shape}\")\n",
    "\n",
    "# --- Feature Engineering ---\n",
    "features_to_lag = ['PRECTOTCORR', 'T2M', 'T2MDEW', 'RH2M', 'WS10M']\n",
    "lag_periods = [1, 2, 3]\n",
    "roll_periods = [3, 5]\n",
    "\n",
    "print(\"Creating lag features (e.g., 'RH2M_lag_1')...\")\n",
    "for col in features_to_lag:\n",
    "    for lag in lag_periods:\n",
    "        new_col_name = f\"{col}_lag_{lag}\"\n",
    "        df[new_col_name] = df[col].shift(lag)\n",
    "\n",
    "print(\"Creating rolling average features (e.g., 'RH2M_roll_avg_3')...\")\n",
    "for col in features_to_lag:\n",
    "    for period in roll_periods:\n",
    "        new_col_name = f\"{col}_roll_avg_{period}\"\n",
    "        df[new_col_name] = df[col].shift(1).rolling(window=period).mean()\n",
    "\n",
    "# --- Data Cleaning ---\n",
    "print(f\"Shape before dropping NaNs: {df.shape}\")\n",
    "df_featured = df.dropna()\n",
    "print(f\"Shape after dropping NaNs: {df_featured.shape}\")\n",
    "\n",
    "# --- Final Verification and Save ---\n",
    "output_filename = \"cloud_burst_final_features.csv\"\n",
    "df_featured.to_csv(output_filename)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(f\"SUCCESS! Your final features dataset is saved as: {output_filename}\")\n",
    "print(\"This single file is now ready for model training.\")\n",
    "print(\"\\nFinal class balance in the new featured dataset:\")\n",
    "print(df_featured['is_cloudburst'].value_counts())\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9a1768-9147-443c-be17-4cd6fc6e5e99",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
